# InsightAudio

**InsightAudio** — полностью локальный сервис для автоматической расшифровки аудио/видео и генерации структурированных пересказов с помощью современных ИИ‑моделей. Все данные обрабатываются и хранятся только на вашем устройстве, гарантируя приватность и безопасность.

---

## Возможности

- Загружайте аудио или видео и автоматически получайте качественную транскрипцию текста.
- Генерируйте структурированный пересказ (summary) с отдельными разделами — удобно для протоколов встреч, лекций, звонков, обучающих материалов.
- Выбирайте и скачивайте различные ИИ‑модели (Whisper, Vosk, Llama, Gemma и др.) для обработки — как для расшифровки речи, так и для summary.
- Используйте разные источники моделей пересказа: локальный Ollama, llama.cpp или собственное HTTP‑API.
- Управляйте всеми константами и наборами моделей через файл `config/default_settings.json`, который можно редактировать без правки исходного кода.
- Получайте summary с указанием таймингов (формат `[HH:MM:SS]`) для каждого ключевого решения.
- Переводите документы DOCX/PPTX/XLSX/PDF и текст на изображениях через Ollama API, выбирая между блочным и монолитным режимами обработки.
- Переключайте архитектуру перевода: пошаговая обработка (точнее) или единый запрос (быстрее, меньше API-вызовов).
- Все модели и конфигурационные файлы хранятся вне Docker-контейнера: легко менять, обновлять, расширять стек.
- Docker-контейнер включает удобный веб-интерфейс (порт 55667) для отслеживания статуса, выбора модели и скачивания результатов.
- Гибкая настройка шаблонов пересказа, конфигурации обработки, поддержка txt, md, docx.
- Тонко настраивайте каждую LLM (контекст, квантизацию, `num_ctx`, `temperature`, `top_p` и др.) через `config/default_settings.json`.
- Максимум приватности: ваши данные под полным контролем, только локально.

---

## Быстрый старт (очередь + SSE)

1. Клонируйте репозиторий InsightAudio и перейдите в его папку.
2. Создайте каталоги `models/` и `config/` (в корне), дайте права на запись.
3. Соберите контейнер:

    ```
    docker-compose build
    ```

4. Запустите web-сервис:

    ```
    docker-compose up
    ```

    Быстрый однострочник (сборка и запуск):

    ```bash
    docker-compose build && docker-compose up
    ```

5. Откройте [http://localhost:55667](http://localhost:55667) — отправляйте задачи. Теперь все задачи выполняются фоном через Celery/Redis, статус обновляется по SSE или опросу `/api/jobs/{id}`.

### Ключевые изменения
- Celery + Redis worker (`docker-compose.yml`) для фоновых задач, TTL cleanup — переменные `CELERY_BROKER_URL`, `CELERY_RESULT_BACKEND`.
- SQLite + SQLAlchemy модели `User`, `Job` (cookie `insight_session`) — хранение статуса, стадии, прогресса, ETA, manifest.
- Новые API: `POST /api/jobs/audio`, `POST /api/jobs/doc_translate`, `GET /api/jobs`, `GET /api/jobs/{id}`, `GET /api/jobs/{id}/download/{asset}`, SSE `/api/jobs/{id}/events`.
- Хранение результатов: `RESULTS_DIR/<user_id>/<job_id>/` с `transcript.txt/json`, `summary.md`, `translation.*`, `meta.json`, manifest в БД.
- UI вкладки: Audio / Documents / My Jobs; SSE обновления; загрузка моделей; prompt templates с редактированием на сессию.
- ASR пресеты (quality/balanced/fast) + advanced (beam, temperature, VAD, no_speech_threshold, loudnorm). Исправлен парсинг Whisper имен (`whisper-large-v3`, `.en` и т.п.).
- Промпты: `config/prompt_templates.json` (meeting, lecture, online_training, interview, sales_call, support_call, brainstorm, standup) и выбор в UI с автоподстановкой в textarea.
- Диаризация: фикс меток без диаризации (все `Speaker 1`), опция в UI.
- Кэш транскрипции учитывает модель, язык, задачу, beam, temperature, VAD, no_speech_threshold, hash файла.

---

## Структура проекта

InsightAudio/
├── app/
│ ├── main.py
│ ├── transcriber.py
│ ├── summarizer.py
│ ├── models.py
│ ├── file_utils.py
│ ├── config_manager.py
│ └── templates/
│ ├── index.html
│ └── results.html
├── Dockerfile
├── requirements.txt
├── docker-compose.yml
├── models/
├── config/
└── README.md




Вот пошаговые команды для запуска InsightAudio и начала работы:

***

1. **Клонируйте или подготовьте директорию InsightAudio:**

```bash
git clone <URL_вашего_репозитория> insightaudio
cd insightaudio
```

2. **Создайте каталоги для моделей и конфигураций (если их нет):**

```bash
mkdir -p models config tmp
```

3. **Соберите Docker-образ (если первый запуск):**

```bash
docker-compose build
```
или
```bash
docker build -t insightaudio .
```

4. **Запустите контейнер с volume и портами (через docker-compose):**

```bash
docker-compose up
```

5. **Проверьте, что сервис стартовал — откройте в браузере:**
```
http://localhost:55667
```

***

**Дополнительно:**

- Для обновления контейнера после изменения кода:
  ```bash
  docker-compose build
  docker-compose up
  ```

- Для удаления остановленного контейнера:
  ```bash
  docker-compose down
  ```

- Каталоги `models/` и `config/` останутся на вашем диске даже при обновлении/удалении контейнера.

---

## Перевод документов и тонкие настройки моделей

- В разделе «Перевод документов» в веб-интерфейсе можно выбрать архитектуру:
  - **Блочный режим** (`DEFAULT_TRANSLATION_MODE = "block"`) — каждый параграф/ячейка переводятся по отдельности. Максимально точно сохраняет структуру и добавляет заметки при переполнении текста.
  - **Единый запрос** (`"full"`) — все текстовые блоки DOCX/PPTX/XLSX собираются в JSON и отправляются одним LLM-вызовом. Это ускоряет работу и снижает нагрузку на Ollama. Для PDF режим автоматически переключается обратно на блочный.
- Значение по умолчанию задаётся в `config/default_settings.json` и может быть переопределено в UI.
- В том же файле находится секция `MODEL_TUNING`, где задаются оптимальные параметры для каждой модели (контекст, квантизация, `num_ctx`, `temperature`, `top_p/top_k`, `num_batch` и т.д.). Эти опции автоматически подмешиваются в запросы переводчика и summarizer’а.

```jsonc
{
  "DEFAULT_TRANSLATION_MODE": "block",
  "MODEL_TUNING": {
    "qwen2.5-coder:32b": {
      "context_length": 32000,
      "quantization": "Q4_K_M",
      "options": {
        "num_ctx": 32000,
        "num_batch": 2,
        "temperature": 0.35,
        "top_p": 0.9
      }
    },
    "deepseek-r1:70b": {
      "context_length": 80000,
      "quantization": "Q4_K_S",
      "options": {
        "num_ctx": 80000,
        "temperature": 0.25,
        "top_p": 0.82
      }
    }
  }
}
```

- Любые дополнительные модели можно добавить в `MODEL_TUNING` тем же способом. Отредактируйте JSON и перезапустите сервис — UI автоматически подхватит новые значения.

***

**После запуска:**
- Используйте веб-интерфейс: загружайте свой аудио/видео файл, выбирайте и скачивайте необходимые модели, запускайте обработку и скачивайте результаты.
- Выбор backend'а для пересказа (Ollama / llama.cpp / Custom API) и пользовательские настройки сохраняются в браузере.
- Все системные логи пишутся в `logs/server.log`, а для каждой обработки создаётся отдельный файл в `logs/requests/<UUID>.log` и блок логов доступен на странице результатов.
- Настройки Ollama API, списки моделей, пути хранения и прочие константы задаются в `config/default_settings.json`. При первом запуске рабочий конфиг `config/config.json` создаётся автоматически с учётом этих значений (по умолчанию Ollama API смотрит на `http://tsnnlx12bs02.ad.telmast.com:11434`).
- Для перевода документов используйте тот же веб-интерфейс: настройте целевой язык и модель (`DEFAULT_TRANSLATE_MODEL`, например `nllb-200:3.3b`), загрузите файл и скачайте переведённый вариант. Если перевод оказывается слишком длинным, сервис автоматически создаёт сокращённый текст и комментарий с полным переводом.

***

Если потребуется добавить Ollama-сервер локально для работы с LLM-саммаризацией, запустите (на хосте):

```bash
ollama serve
```
и скачайте нужные модели:
```bash
ollama pull llama3:8b
```

***

Все готово к работе! Если нужна тестовая демонстрация или любой файл — запросите отдельно.


docker-compose down;docker-compose build;docker-compose up -d