# InsightAudio

**InsightAudio** — полностью локальный сервис для автоматической расшифровки аудио/видео и генерации структурированных пересказов с помощью современных ИИ‑моделей. Все данные обрабатываются и хранятся только на вашем устройстве, гарантируя приватность и безопасность.

---

## Возможности

- Загружайте аудио или видео и автоматически получайте качественную транскрипцию текста.
- Генерируйте структурированный пересказ (summary) с отдельными разделами — удобно для протоколов встреч, лекций, звонков, обучающих материалов.
- Выбирайте и скачивайте различные ИИ‑модели (Whisper, Vosk, Llama, Gemma и др.) для обработки — как для расшифровки речи, так и для summary.
- Используйте разные источники моделей пересказа: локальный Ollama, llama.cpp или собственное HTTP‑API.
- Управляйте всеми константами и наборами моделей через файл `config/default_settings.json`, который можно редактировать без правки исходного кода.
- Получайте summary с указанием таймингов (формат `[HH:MM:SS]`) для каждого ключевого решения.
- Переводите документы DOCX/PPTX/XLSX/PDF и текст на изображениях через Ollama API, выбирая между блочным и монолитным режимами обработки.
- Переключайте архитектуру перевода: пошаговая обработка (точнее) или единый запрос (быстрее, меньше API-вызовов).
- Все модели и конфигурационные файлы хранятся вне Docker-контейнера: легко менять, обновлять, расширять стек.
- Docker-контейнер включает удобный веб-интерфейс (порт 55667) для отслеживания статуса, выбора модели и скачивания результатов.
- Гибкая настройка шаблонов пересказа, конфигурации обработки, поддержка txt, md, docx.
- Тонко настраивайте каждую LLM (контекст, квантизацию, `num_ctx`, `temperature`, `top_p` и др.) через `config/default_settings.json`.
- Максимум приватности: ваши данные под полным контролем, только локально.

---

## Быстрый старт (очередь + SSE)

1. Клонируйте репозиторий InsightAudio и перейдите в его папку.
2. Создайте каталоги `models/` и `config/` (в корне), дайте права на запись.
3. Соберите контейнер:

    ```
    docker-compose build
    ```

4. Запустите web-сервис:

    ```
    docker-compose up
    ```

    Быстрый однострочник (сборка и запуск):

    ```bash
    docker-compose build && docker-compose up
    ```

5. Откройте [http://localhost:55667](http://localhost:55667) — отправляйте задачи. Теперь все задачи выполняются фоном через Celery/Redis, статус обновляется по SSE или опросу `/api/jobs/{id}`.

### Архитектура и ключевые изменения

#### Фоновая обработка задач
- **Celery + Redis**: Все задачи (транскрипция, пересказ, перевод) выполняются в фоне через Celery worker
- **SSE (Server-Sent Events)**: Реальное время обновления статуса задач без polling
- **Job Queue**: Задачи продолжают выполняться даже при закрытии вкладки браузера
- **TTL Cleanup**: Автоматическая очистка устаревших jobs и файлов через Celery beat (настраивается через `JOB_TTL_DAYS`)

#### База данных и сессии
- **SQLite + SQLAlchemy**: Модели `User` и `Job` для хранения состояния задач
- **Cookie сессии**: `insight_session` (UUID, HttpOnly, SameSite=Lax, 90 дней)
- **SQLite WAL mode**: Включен для улучшенной конкурентности
- **Абсолютные пути**: Гарантирована стабильность путей между web/worker

#### API Endpoints
- `POST /api/jobs/audio` — создание задачи транскрипции аудио/видео
- `POST /api/jobs/audio/batch` — создание нескольких задач транскрипции за один запрос (FormData: `files[]`, `params`)
- `POST /api/jobs/doc_translate` — создание задачи перевода документа
- `GET /api/jobs` — список задач текущего пользователя
- `GET /api/jobs/{id}` — детали задачи
- `GET /api/jobs/{id}/download/{asset_name}` — безопасное скачивание файла из задачи
- `GET /api/jobs/{id}/events` — SSE поток событий задачи
- `POST /api/check_custom_api` — проверка подключения к пользовательскому API

#### Хранение результатов
- **Структура**: `RESULTS_DIR/<user_id>/<job_id>/`
- **Файлы**: `input_original.*`, `audio.wav`, `transcript.txt`, `transcript.json`, `summary.md`, `translation.*`, `meta.json`
- **Manifest**: Полная информация о файлах (имя, тип, размер, дата создания) в БД
- **Безопасность**: Проверка ownership, manifest и защита от path traversal

#### ASR (Automatic Speech Recognition)
- **Движки**: `auto` (автовыбор), `faster-whisper` (рекомендуется), `openai-whisper` (legacy)
- **Модели**: Whisper (tiny, base, small, medium, large, large-v2, large-v3), Vosk Russian
- **Пресеты**: `quality` (beam=5, temperature=0.0, vad_filter=true, loudnorm=true), `balanced` (beam=3), `fast` (beam=1)
- **Advanced параметры**: beam_size, temperature, vad_filter (только faster-whisper), no_speech_threshold, loudnorm
- **Кэширование**: Учитывает модель, движок, язык, beam, temperature, VAD, hash файла (TTL 7 дней)
- **Прогресс/ETA**: Честный прогресс для faster-whisper (обновление каждые ≤2 сек), RTF-based ETA
- **Ограничения безопасности**: Лимит длительности файла (`MAX_AUDIO_DURATION_SEC`, по умолчанию 14400 секунд) и таймаут ffmpeg-конвертации (`FFMPEG_TIMEOUT_SECONDS`) защищают воркеры от зависаний и слишком длинных медиа

#### Пересказ (Summary)
- **Backend**: Ollama, llama.cpp, Custom API
- **Промпты**: Шаблоны из `config/prompt_templates.json` (meeting, lecture, interview, customer_call, standup, brainstorm, support_call, online_training)
- **Чанкинг**: Автоматическое разбиение длинных транскрипций на чанки с последующим reduce
- **Ревью**: Опциональная проверка/улучшение пересказа через отдельную модель
- **Параметры**: temperature, top_p, max_tokens, num_ctx (из MODEL_TUNING)

#### Переводчик документов
- **Форматы**: DOCX, PPTX, XLSX, PDF, TXT
- **Режимы**: Блочный (точнее) или единый запрос (быстрее)
- **PDF Reflow**: Опция переформатирования PDF перед переводом
- **Перевод изображений**: Режимы "notes" (заметки) или "full" (полный перевод)

#### UI
- **Вкладки**: Audio, Documents, My Jobs
- **SSE обновления**: Автоматическое обновление статуса задач в реальном времени
- **Загрузка моделей**: Проверка статуса и скачивание моделей через UI
- **Prompt templates**: Выбор шаблона с автоподстановкой и возможностью редактирования
- **Динамическое UI**: Скрытие/показ блоков в зависимости от выбранных опций

#### Безопасность
- **Проверка ownership**: Пользователь может скачивать только свои файлы
- **Manifest validation**: Файлы проверяются по manifest перед скачиванием
- **Path traversal protection**: Защита от `../` и других попыток доступа к файлам вне job_dir
- **Chunked upload**: Потоковая загрузка файлов чанками (8MB) без чтения всего файла в память
- **Size limits**: Настраиваемый лимит размера файла (`MAX_UPLOAD_MB`)
- **Batch limits**: Количество файлов в мультизагрузке ограничено (`MAX_AUDIO_BATCH_FILES`)

#### GPU поддержка
- **CUDA**: Автоматическое определение и использование GPU если доступно
- **Fallback**: Автоматический переход на CPU если CUDA недоступна
- **Конфигурация**: Управление через `USE_CUDA` в настройках
- **Docker GPU**: Поддержка NVIDIA GPU через `docker-compose.yml` (раскомментируйте секцию `deploy`)

#### Логирование
- **Ротация логов**: `logs/server.log` с автоматической ротацией и архивацией
- **Request logs**: Отдельные логи для каждой задачи в `logs/requests/<UUID>.log`
- **Уровни**: DEBUG, INFO, WARNING, ERROR с настраиваемыми уровнями

---

## Структура проекта

InsightAudio/
├── app/
│ ├── main.py
│ ├── transcriber.py
│ ├── summarizer.py
│ ├── models.py
│ ├── file_utils.py
│ ├── config_manager.py
│ └── templates/
│ ├── index.html
│ └── results.html
├── Dockerfile
├── requirements.txt
├── docker-compose.yml
├── models/
├── config/
└── README.md




Вот пошаговые команды для запуска InsightAudio и начала работы:

***

1. **Клонируйте или подготовьте директорию InsightAudio:**

```bash
git clone <URL_вашего_репозитория> insightaudio
cd insightaudio
```

2. **Создайте каталоги для моделей и конфигураций (если их нет):**

```bash
mkdir -p models config tmp
```

3. **Соберите Docker-образ (если первый запуск):**

```bash
docker-compose build
```
или
```bash
docker build -t insightaudio .
```

4. **Запустите контейнер с volume и портами (через docker-compose):**

```bash
docker-compose up
```

5. **Проверьте, что сервис стартовал — откройте в браузере:**
```
http://localhost:55667
```

***

**Дополнительно:**

- Для обновления контейнера после изменения кода:
  ```bash
  docker-compose build
  docker-compose up
  ```

- Для удаления остановленного контейнера:
  ```bash
  docker-compose down
  ```

- Каталоги `models/` и `config/` останутся на вашем диске даже при обновлении/удалении контейнера.

---

## Перевод документов и тонкие настройки моделей

- В разделе «Перевод документов» в веб-интерфейсе можно выбрать архитектуру:
  - **Блочный режим** (`DEFAULT_TRANSLATION_MODE = "block"`) — каждый параграф/ячейка переводятся по отдельности. Максимально точно сохраняет структуру и добавляет заметки при переполнении текста.
  - **Единый запрос** (`"full"`) — все текстовые блоки DOCX/PPTX/XLSX собираются в JSON и отправляются одним LLM-вызовом. Это ускоряет работу и снижает нагрузку на Ollama. Для PDF режим автоматически переключается обратно на блочный.
- Значение по умолчанию задаётся в `config/default_settings.json` и может быть переопределено в UI.
- В том же файле находится секция `MODEL_TUNING`, где задаются оптимальные параметры для каждой модели (контекст, квантизация, `num_ctx`, `temperature`, `top_p/top_k`, `num_batch` и т.д.). Эти опции автоматически подмешиваются в запросы переводчика и summarizer’а.

```jsonc
{
  "DEFAULT_TRANSLATION_MODE": "block",
  "MODEL_TUNING": {
    "qwen2.5-coder:32b": {
      "context_length": 32000,
      "quantization": "Q4_K_M",
      "options": {
        "num_ctx": 32000,
        "num_batch": 2,
        "temperature": 0.35,
        "top_p": 0.9
      }
    },
    "deepseek-r1:70b": {
      "context_length": 80000,
      "quantization": "Q4_K_S",
      "options": {
        "num_ctx": 80000,
        "temperature": 0.25,
        "top_p": 0.82
      }
    }
  }
}
```

- Любые дополнительные модели можно добавить в `MODEL_TUNING` тем же способом. Отредактируйте JSON и перезапустите сервис — UI автоматически подхватит новые значения.

***

**После запуска:**
- Используйте веб-интерфейс: загружайте свой аудио/видео файл, выбирайте и скачивайте необходимые модели, запускайте обработку и скачивайте результаты.
- Выбор backend'а для пересказа (Ollama / llama.cpp / Custom API) и пользовательские настройки сохраняются в браузере.
- Все системные логи пишутся в `logs/server.log`, а для каждой обработки создаётся отдельный файл в `logs/requests/<UUID>.log` и блок логов доступен на странице результатов.
- Настройки Ollama API, списки моделей, пути хранения и прочие константы задаются в `config/default_settings.json`. При первом запуске рабочий конфиг `config/config.json` создаётся автоматически с учётом этих значений (по умолчанию Ollama API смотрит на `http://localhost:11434`).
- Для перевода документов используйте тот же веб-интерфейс: настройте целевой язык и модель (`DEFAULT_TRANSLATE_MODEL`, например `nllb-200:3.3b`), загрузите файл и скачайте переведённый вариант. Если перевод оказывается слишком длинным, сервис автоматически создаёт сокращённый текст и комментарий с полным переводом.

***

Если потребуется добавить Ollama-сервер локально для работы с LLM-саммаризацией, запустите (на хосте):

```bash
ollama serve
```
и скачайте нужные модели:
```bash
ollama pull llama3:8b
```

***

Все готово к работе! Если нужна тестовая демонстрация или любой файл — запросите отдельно.


docker-compose down;docker-compose build;docker-compose up -d
